# -*- coding: utf-8 -*-
"""lab1_nlp

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1TPUy74YG8nLbqc3iY_ryGQyupiOz04Cp
"""

!pip install gensim

import numpy as np
import pandas as pd
from nltk.tokenize import word_tokenize
from gensim.models import Word2Vec
from sklearn.model_selection import train_test_split
from sklearn.manifold import TSNE
import matplotlib.pyplot as plt

import torch

data = pd.read_csv('train_en.txt', sep='\t')

data

"""Task 1"""

import nltk
nltk.download('punkt_tab')

data['Tokens'] = data['Sentence'].apply(lambda x:word_tokenize(x))

data

sentences = data['Tokens'].values.tolist()

sentences

"""*SkipGram*  (sg=1)
За секој збор го предвидува контекстот (околни зборови)
Подобро учи ретки зборови и позначајни семантички односи
"""



word2vec_model = Word2Vec(sentences, vector_size=100, window=3, min_count=15, sg=1)

len(word2vec_model.wv.index_to_key)

len(word2vec_model.wv.vectors)

words = word2vec_model.wv.index_to_key

word_vectors = word2vec_model.wv.vectors

tsne = TSNE(n_components=2)

word_vectors_2d = tsne.fit_transform(word_vectors[:50])

plt.scatter(word_vectors_2d[:, 0], word_vectors_2d[:, 1])
for label, x, y in zip(words[:50], word_vectors_2d[:, 0], word_vectors_2d[:, 1]):
    plt.annotate(label, xy=(x, y), xytext=(0, 0), textcoords='offset points')
plt.show()

wv = word2vec_model.wv

v = wv["Paris"] - wv["France"] + wv["Italy"]
nearest = wv.similar_by_vector(v, topn=10)
print(nearest[0])

res = wv.most_similar(positive=["Paris", "Italy"], negative=["France"], topn=10)
print(res[0])

v =  wv["Spain"] + wv["France"]
nearest = wv.similar_by_vector(v, topn=10)
print(nearest[0])

res = wv.most_similar(positive=[ "France"], negative=["Spain"], topn=10)
print(res[0])

v = wv["King"] - wv["Man"] + wv["Woman"]
nearest = wv.similar_by_vector(v, topn=10)
print(nearest[0])

res = wv.most_similar(positive=["King", "Woman"], negative=["Man"], topn=10)
print(res[0])

v = wv["Bigger"] - wv["Big"] + wv["Cold"]
nearest = wv.similar_by_vector(v, topn=10)
print(nearest[0])

res = wv.most_similar(positive=["Bigger", "Cold"], negative=["Big"], topn=10)
print(res[0])

v = wv["Windows"]+ wv["Google"]
nearest = wv.similar_by_vector(v, topn=10)
print(nearest[0])

res = wv.most_similar(positive=["Windows", "Google"], topn=10)
print(res[0])

"""*CBOW*
(sg=0)
Го користи контекстот за да го предвиди централниот збор
Побрз, подобар за чести зборови, но понекогаш губи финa смисла
"""

word2vec_model_cbow = Word2Vec(sentences, vector_size=200, window=5, min_count=15, sg=0)

wv_cbow = word2vec_model_cbow.wv

word_vectors_cbow = word2vec_model_cbow.wv.vectors

words_cbow = word2vec_model_cbow.wv.index_to_key

word_vectors_2d_cbow = tsne.fit_transform(word_vectors_cbow[:50])

plt.scatter(word_vectors_2d_cbow[:, 0], word_vectors_2d_cbow[:, 1])
for label, x, y in zip(words_cbow[:50], word_vectors_2d_cbow[:, 0], word_vectors_2d_cbow[:, 1]):
    plt.annotate(label, xy=(x, y), xytext=(0, 0), textcoords='offset points')
plt.show()

res = wv_cbow.most_similar(positive=["Paris", "Italy"], negative=["France"], topn=10)
print(res[0])

"""Task 2"""

import torch.nn as nn
from torch.nn import Sequential
from torch.nn import Embedding

PAD = "<PAD>"
UNK = "<UNK>"

vocab = [PAD,UNK] + wv.index_to_key
word2idx = {w:i for i,w in enumerate(vocab)}

pad_idx = word2idx[PAD]
unk_idx = word2idx[UNK]

emb_dim = wv.vectors.shape[1]
emb_dim

emb_matrix = np.zeros((len(vocab), emb_dim), dtype=np.float32)
emb_matrix

emb_matrix[unk_idx] = wv.vectors.mean(axis=0)

emb_matrix[2:] = wv.vectors

def encode(token_list):
  return [word2idx.get(tok, unk_idx) for tok in token_list]

X_idx = [encode(s) for s in sentences]

X_idx

lengths = [len(x) for x in X_idx if len(x) >0]
avg_len = int(round(sum(lengths)/len(lengths))) if lengths else 32
max_len = max(8, avg_len)

from torch.nn.utils.rnn import pad_sequence

from tensorflow.keras.preprocessing.sequence import pad_sequences

X_pad = pad_sequences(X_idx, maxlen=max_len, padding="post", value=pad_idx)

X_pad

labels = [1 if label=='positive' else 0 for label in data['Style']]
labels

from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import LSTM, GRU, Dense, Dropout, Embedding
from tensorflow.keras.preprocessing.sequence import pad_sequences
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.losses import binary_crossentropy

model = Sequential()

model.add(Embedding(input_dim=len(vocab), output_dim=100, weights=[emb_matrix], trainable=True, mask_zero=True))

model.add(LSTM(128))

model.add(Dense(1))

model.compile(optimizer='sgd', loss='binary_crossentropy')

model.summary()

model.fit(X_pad, np.array(labels), epochs=3)

X_test = pd.read_csv('test_en.txt', sep='\t')
X_test

test_sentences = X_test['Sentence'].apply(lambda x:word_tokenize(x))

X_test_idx = [encode(s) for s in test_sentences]

X_test_idx

X_test_pad = pad_sequences(X_test_idx, maxlen=max_len, padding="post", truncating="post",
                           value=word2idx["<PAD>"])

y_pred = model.predict(X_test_pad)

y_pred_fin = (y_pred >= 0.5).astype(int)

y_test = [1 if label=='positive' else 0 for label in X_test['Style']]
y_test_arr = np.asarray(y_test,dtype=int)
y_test_arr

from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score

acc  = accuracy_score(y_test_arr, y_pred_fin)
prec = precision_score(y_test_arr, y_pred_fin, zero_division=0)
rec  = recall_score(y_test_arr, y_pred_fin, zero_division=0)
f1   = f1_score(y_test_arr, y_pred_fin, zero_division=0)

print(f"accuracy={acc:.4f}  precision={prec:.4f}  recall={rec:.4f}  f1={f1:.4f}")

from sklearn.metrics import classification_report
classification_report(y_test_arr, y_pred_fin, labels=[0,1])

