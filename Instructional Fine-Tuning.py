# -*- coding: utf-8 -*-
"""lab3.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1dQ4UfeuFg4658yVlrnkJhX34JQRkSTFj
"""

import pandas as pd
from sklearn.model_selection import train_test_split

import torch
from torch.utils.data import Dataset

from datasets import Dataset

df = pd.read_csv('test_en_parallel.txt', sep='\t')

df.head()

len(df)

train_df, test_df = train_test_split(df, test_size=0.2, random_state=42)

def create_transformers_train_data(sentences, translations, tokenizer):
    inputs_ne = tokenizer(sentences, max_length=64, truncation=True, padding="max_length")

    with tokenizer.as_target_tokenizer():
        outputs_po = tokenizer(translations, max_length=64, truncation=True, padding="max_length")

    labels=outputs_po["input_ids"]

    pad_id = tokenizer.pad_token_id

    new_labels = []
    for seq in labels:
        new_seq = []
        for token_id in seq:
            if token_id == pad_id:
                new_seq.append(-100)
            else:
                new_seq.append(token_id)
        new_labels.append(new_seq)


    data = Dataset.from_dict({'input_ids': inputs_ne['input_ids'],
                              'attention_mask': inputs_ne['attention_mask'],
                              'labels': outputs_po['input_ids']})
    return data

from transformers import AutoTokenizer, AutoModelForSeq2SeqLM

"""**Task 1**

EncoderDecoder T5 Model
"""

tokenizer = AutoTokenizer.from_pretrained("t5-small")

train_dataset = create_transformers_train_data(
    train_df["Style 1"].tolist(),
    train_df["Style 2"].tolist(),
    tokenizer
)

test_dataset = create_transformers_train_data(
    test_df["Style 1"].tolist(),
    test_df["Style 2"].tolist(),
    tokenizer
)

model = AutoModelForSeq2SeqLM.from_pretrained("t5-small")

class Neg2PosDataset(Dataset):
    def __init__(self, encodings, labels):
        self.encodings = encodings
        self.labels = labels

    def __getitem__(self, idx):
        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}
        item['labels'] = torch.tensor(self.labels[idx])
        return item

    def __len__(self):
        return len(self.labels)

from transformers import Trainer, TrainingArguments

training_args = TrainingArguments(
    output_dir="./neg2pos",
    num_train_epochs=3,
    per_device_train_batch_size=8,
    per_device_eval_batch_size=8,
    save_strategy="epoch",
    logging_steps=50,
    lr_scheduler_type="linear",
    learning_rate=5e-5,
    weight_decay=0.01,
    report_to="none"
)

trainer = Trainer(
    model=model,
    args = training_args,
    train_dataset=train_dataset,
    eval_dataset=test_dataset
)

trainer.train()

model.eval()

test_inputs = test_df["Style 1"].tolist()
references = test_df["Style 2"].tolist()

device = next(model.parameters()).device
predictions = []

for input in test_inputs:
  encoded = tokenizer(input, return_tensors="pt", truncation=True, padding=True, max_length=64)

  input_ids = encoded["input_ids"].to(device)
  attention_mask = encoded["attention_mask"].to(device)

  with torch.no_grad():
    outputs = model.generate(input_ids, max_length=64, num_beams=4)

  pred = tokenizer.decode(outputs[0], skip_special_tokens=True)
  predictions.append(pred)

pip install sacrebleu

import sacrebleu

bleu = sacrebleu.corpus_bleu(predictions,[references])

print("BLEU:", bleu.score)

pip install bert-score

from bert_score import score

P, R, F1 = score(predictions, references, lang="en")
print("BERTScore: ")
print("Precision:", P.mean().item())
print("Recall:", R.mean().item())
print("F1:", F1.mean().item())

"""EncoderDecoder FLAN-T5"""

tokenizer_flan = AutoTokenizer.from_pretrained("google/flan-t5-small")

train_dataset_flan = create_transformers_train_data(
    train_df["Style 1"].tolist(),
    train_df["Style 2"].tolist(),
    tokenizer_flan
)

test_dataset_flan = create_transformers_train_data(
    test_df["Style 1"].tolist(),
    test_df["Style 2"].tolist(),
    tokenizer_flan
)

model_flan = AutoModelForSeq2SeqLM.from_pretrained("google/flan-t5-small")

trainer_flan = Trainer (
    model = model_flan,
    args = training_args,
    train_dataset= train_dataset_flan,
    eval_dataset=test_dataset_flan
)

trainer_flan.train()

model_flan.eval()

device = next(model_flan.parameters()).device

predictions_flan = []

for input in test_inputs:
  encoded = tokenizer_flan(input, return_tensors="pt", truncation=True, padding=True, max_length=64)

  input_ids = encoded["input_ids"].to(device)
  attention_mask = encoded["attention_mask"].to(device)

  with torch.no_grad():
    ouputs = model_flan.generate(input_ids, max_length=64)

  pred=tokenizer_flan.decode(ouputs[0], skip_special_tokens=True)
  predictions_flan.append(pred)

bleu_flan = sacrebleu.corpus_bleu(predictions_flan, [references])

print("BLEU FLAN-5", bleu_flan.score)

P, R, F1 = score(predictions_flan, references, lang="en")
print("BERTScore FLAN-5: ")
print("Precision:", P.mean().item())
print("Recall:", R.mean().item())
print("F1:", F1.mean().item())

"""* Со моделот T5 се добива повисок BLEU score, а со моделот FLAN-5 се добива повисок BERTScore, а тоа значи дека моделот T5 подобро ги погодува истите зборови, а моделот FLAN-5 подобро го погодува семантичкото значењето на речениците.

**Task 2**
"""

def make_instruction(neg_sentence):
    return (
        "Instruction: Rewrite the following sentence so that it has a positive sentiment.\n"
        f"Input: {neg_sentence}\n"
        "Output:"
    )

train_df = train_df.dropna(subset=["Style 1", "Style 2"])
test_df = test_df.dropna(subset=["Style 1", "Style 2"])

train_df["Instruction"] = train_df["Style 1"].apply(make_instruction)
test_df["Instruction"]=test_df["Style 1"].apply(make_instruction)

tokenizer = AutoTokenizer.from_pretrained("t5-small")

train_dataset = create_transformers_train_data(
    train_df["Instruction"].tolist(),
    train_df["Style 2"].tolist(),
    tokenizer
)

test_dataset = create_transformers_train_data(
    test_df["Instruction"].tolist(),
    test_df["Style 2"].tolist(),
    tokenizer
)

training_args = TrainingArguments(
        output_dir="./neg2pos_t5_instructional",
    num_train_epochs=3,
    per_device_train_batch_size=8,
    per_device_eval_batch_size=8,
    save_strategy="epoch",
    logging_steps=50,
    learning_rate=5e-5,
    weight_decay=0.01,
    lr_scheduler_type="linear",
    report_to="none"
)

model = AutoModelForSeq2SeqLM.from_pretrained("t5-small")

trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=train_dataset,
    eval_dataset=test_dataset
)

trainer.train()

model.eval()

test_inputs = test_df["Style 1"].tolist()
references  = test_df["Style 2"].tolist()

predictions = []

for neg in test_inputs:
    instr = make_instruction(neg)

    encoded = tokenizer(
        instr,
        return_tensors="pt",
        truncation=True,
        padding=True,
        max_length=64
    )

    input_ids = encoded["input_ids"].to(device)
    attention_mask = encoded["attention_mask"].to(device)

    with torch.no_grad():
        outputs = model.generate(
            input_ids=input_ids,
            attention_mask=attention_mask,
            max_length=64,
            num_beams=4
        )

    pred = tokenizer.decode(outputs[0], skip_special_tokens=True)
    predictions.append(pred)

bleu = sacrebleu.corpus_bleu(predictions, [references])
print("BLEU:", bleu.score)

P, R, F1 = score(predictions, references, lang="en")
print("BERTScore:")
print("Precision:", P.mean().item())
print("Recall:", R.mean().item())
print("F1:", F1.mean().item())

"""* Со моделот T5 се добиваат многу ложи резултати за инструкциско fine-tuning што е очекувано бидејќи тој модел не е instruction модел.

Transform negative to positive sentence (with instructional fine_tuning model)
"""

tokenizer_flan = AutoTokenizer.from_pretrained("google/flan-t5-small")

model_flan = AutoModelForSeq2SeqLM.from_pretrained("google/flan-t5-small")

train_dataset = create_transformers_train_data(
    train_df["Instruction"].tolist(),
    train_df["Style 2"].tolist(),
    tokenizer_flan
)

test_dataset = create_transformers_train_data(
    test_df["Instruction"].tolist(),
    test_df["Style 2"].tolist(),
    tokenizer_flan
)

trainer = Trainer(
    model=model_flan,
    args=training_args,
    train_dataset=train_dataset,
    eval_dataset=test_dataset
)

trainer.train()

predictions = []

for neg in test_inputs:
    instr = make_instruction(neg)

    encoded = tokenizer_flan(
        instr,
        return_tensors="pt",
        truncation=True,
        padding=True,
        max_length=64
    )

    input_ids = encoded["input_ids"].to(device)
    attention_mask = encoded["attention_mask"].to(device)

    with torch.no_grad():
        outputs = model_flan.generate(
            input_ids=input_ids,
            attention_mask=attention_mask,
            max_length=64,
            num_beams=4
        )

    pred = tokenizer_flan.decode(outputs[0], skip_special_tokens=True)
    predictions.append(pred)

bleu = sacrebleu.corpus_bleu(predictions, [references])
print("BLEU:", bleu.score)

P, R, F1 = score(predictions, references, lang="en")
print("BERTScore:")
print("Precision:", P.mean().item())
print("Recall:", R.mean().item())
print("F1:", F1.mean().item())

"""* Со моделот FLAN-T5 кој е instruction модел се добиваат подобри резултати со инструкција како влез во моделот за трансформирање на реченицата од негативна во позитивна.

Task 3
"""

def multitask_transform_df(df, col_neg="Style 1", col_pos="Style 2"):
  rows = []
  for _, r in df.iterrows():
    neg = r[col_neg]
    pos = r[col_pos]
    if pd.isna(neg) or pd.isna(pos):
      continue

    rows.append({
        "task": "rewrite",
        "input_text" : (
            "Instruction: Rewrite the following sentence so that it has a positive sentiment.\n"
            f"Input: {neg}\n"
            "Output:"
        ),
        "target_text":str(pos)
    })

    rows.append({
        "task": "classify",
        "input_text":(
            "Instruction: Decide whether the sentiment of the following sentence is positive or negative.\n"
            f"Sentence: {neg}\n"
            "Answer:"
        ),
        "target_text":"negative"
    })

    rows.append({
        "task":"classify",
        "input_text":(
            "Instruction: Decide whether the sentiment of the following sentence is positive or negative.\n"
            f"Sentence: {pos}\n"
            "Answer:"
        ),
        "target_text":"positive"
    })

  return pd.DataFrame(rows)

df = df.dropna(subset=["Style 1", "Style 2"])

df_mulitask = multitask_transform_df(df, "Style 1", "Style 2")

df_mulitask.head(3)

df_mulitask.loc[0, "input_text"]

df_mulitask.loc[0,"target_text"]

train_df, test_df = train_test_split(df_mulitask, test_size=0.2, random_state=42, stratify=df_mulitask["task"])

tokenizer = AutoTokenizer.from_pretrained("t5-small")

train_dataset = create_transformers_train_data(
    train_df["input_text"].tolist(),
    train_df["target_text"].tolist(),
    tokenizer
)

test_dataset = create_transformers_train_data(
    test_df["input_text"].tolist(),
    test_df["target_text"].tolist(),
    tokenizer
)

model = AutoModelForSeq2SeqLM.from_pretrained("t5-small")

training_args = TrainingArguments(
    output_dir="./t5_multitask_instructional",
    num_train_epochs=3,
    per_device_train_batch_size=8,
    per_device_eval_batch_size=8,
    save_strategy="epoch",
    logging_steps=50,
    learning_rate=5e-5,
    weight_decay=0.01,
    lr_scheduler_type="linear",
    report_to="none"
)

trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=train_dataset,
    eval_dataset=test_dataset
)

trainer.train()

"""Evaluation for transformation negative -> positive"""

rewrite_test = test_df[test_df["task"]=="rewrite"].reset_index(drop=True)

rewrite_inputs = rewrite_test["input_text"].tolist()
rewrite_refs = rewrite_test["target_text"].tolist()

model.eval()

device = next(model.parameters()).device

rewrite_preds = []
for ri in rewrite_inputs:
  enc = tokenizer(ri, return_tensors="pt", truncation=True, padding=True,max_length=64)
  input_ids = enc["input_ids"].to(device)
  attn = enc["attention_mask"].to(device)

  with torch.no_grad():
    out = model.generate(input_ids=input_ids, attention_mask=attn, max_length=64)

  rewrite_preds.append(tokenizer.decode(out[0], skip_special_tokens=True))

bleu = sacrebleu.corpus_bleu(rewrite_preds, [rewrite_refs])
print("BLEU (rewrite):", bleu.score)

P, R, F1 = score(rewrite_preds, rewrite_refs, lang="en")
print("BERTScore F1 (rewrite):", F1.mean().item())

"""Evaluation for classification positive/negative"""

cls_test = test_df[test_df["task"] == "classify"].reset_index(drop=True)

cls_inputs = cls_test["input_text"].tolist()
cls_true   = cls_test["target_text"].tolist()

cls_preds_raw = []
for inp in cls_inputs:
    enc = tokenizer(inp, return_tensors="pt", truncation=True, padding=True, max_length=128)
    input_ids = enc["input_ids"].to(device)
    attn = enc["attention_mask"].to(device)

    with torch.no_grad():
        out = model.generate(input_ids=input_ids, attention_mask=attn, max_length=5, num_beams=4)

    cls_preds_raw.append(tokenizer.decode(out[0], skip_special_tokens=True).strip().lower())

def norm_label(x):
    x = x.lower()
    if "positive" in x: return "positive"
    if "negative" in x: return "negative"
    return "negative"

cls_pred = [norm_label(x) for x in cls_preds_raw]

accuracy = sum(p == t for p, t in zip(cls_pred, cls_true)) / len(cls_true)
print("Accuracy (classify):", accuracy)

"""* Се добиваат одлични резултати за
класификација и препознавање дали одредена реченица содржи позитивен или негативен сентимент.

* Со мултитаск се добиваат и подобри резултати за трансформација на речениците од негативни во позитивни со моделот T5 поради тоа што моделот гледа и примери од класификацијата која му помага на моделот да разликува позитивно од негативно.
"""

цол